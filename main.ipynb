{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(os.path.abspath(\".\"), \"data\")\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "TEST_DIR = os.path.join(DATA_DIR, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train features: 0\n",
      "Number of train masks: 0\n"
     ]
    }
   ],
   "source": [
    "train_features = [f for f in os.listdir(TRAIN_DIR) if f.endswith(\".png\") and not f.endswith(\"_GT.png\")]\n",
    "train_masks = [f for f in os.listdir(TRAIN_DIR) if f.endswith(\"_GT.png\")]\n",
    "\n",
    "print(\"Number of train features:\", len(train_features))\n",
    "print(\"Number of train masks:\", len(train_masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset contains one copied sample with (copy) in name -> identify and remove\n",
    "[i for i in train_features if i.split(\".\")[0] + \"_GT.png\" not in train_masks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing Directories\n",
    "current structure:  `data/train` & `data/test`\n",
    "\n",
    "split both train and test based on whether they are defective or not\n",
    "\n",
    "i.e `data/*/defect` & `data/*/no_defect`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_defective(dirname):\n",
    "    images = [f.split(\".\")[0] for f in os.listdir(dirname) if f.endswith(\".png\") and not f.endswith(\"_GT.png\")]\n",
    "    for img in images:\n",
    "        mask = np.array(Image.open(os.path.join(dirname, img + \"_GT.png\")))\n",
    "        if mask.sum() > 0:\n",
    "            os.rename(os.path.join(dirname, img + \".png\"),os.path.join(dirname, \"defect\", img + \".png\"))\n",
    "            os.rename(os.path.join(dirname, img + \"_GT.png\"),os.path.join(dirname, \"defect\", img + \"_GT.png\"))\n",
    "        else:\n",
    "            os.rename(os.path.join(dirname, img + \".png\"),os.path.join(dirname, \"no_defect\", img + \".png\"))\n",
    "            os.rename(os.path.join(dirname, img + \"_GT.png\"),os.path.join(dirname, \"no_defect\", img + \"_GT.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_defective(TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of defective images in TRAIN: 246\n",
      "Number of non-defective images in TRAIN: 894\n",
      "Number of defective images in TEST: 110\n",
      "Number of non-defective images in TEST: 894\n"
     ]
    }
   ],
   "source": [
    "num_train_defect = len([f for f in os.listdir(TRAIN_DIR + \"\\\\defect\") if not f.endswith(\"_GT.png\")])\n",
    "num_train_no_defect = len([f for f in os.listdir(TRAIN_DIR + \"\\\\no_defect\") if not f.endswith(\"_GT.png\")])\n",
    "\n",
    "num_test_defect = len([f for f in os.listdir(TEST_DIR + \"\\\\defect\") if not f.endswith(\"_GT.png\")])\n",
    "num_test_no_defect = len([f for f in os.listdir(TEST_DIR + \"\\\\no_defect\") if not f.endswith(\"_GT.png\")])\n",
    "\n",
    "print(\"Number of defective images in TRAIN:\", num_train_defect)\n",
    "print(\"Number of non-defective images in TRAIN:\", num_test_no_defect)\n",
    "print(\"Number of defective images in TEST:\", num_test_defect)\n",
    "print(\"Number of non-defective images in TEST:\", num_test_no_defect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_defective(TEST_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefectDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, mask_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform\n",
    "        self.classes = [\"no_defect\", \"defect\"]  # Ensures labels are assigned correctly\n",
    "\n",
    "        self.samples = []\n",
    "        for label, class_name in enumerate(self.classes):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                if img_name.endswith(\".png\") and not img_name.endswith(\"_GT.png\"):\n",
    "                    img_path = os.path.join(class_dir, img_name)\n",
    "                    mask_path = os.path.join(class_dir, img_name.replace(\".png\", \"_GT.png\"))\n",
    "                    self.samples.append((img_path, mask_path, label))  # Store image, mask, and label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, mask_path, label = self.samples[idx]\n",
    "\n",
    "        # Open images\n",
    "        image = Image.open(img_path).convert(\"RGB\")  # Feature image\n",
    "        mask = Image.open(mask_path).convert(\"L\")   # Mask (grayscale)\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "\n",
    "        return image, mask, label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Weighted Sampler to assign weights to each sample based on class frequency\n",
    "def get_sampler(dataset):\n",
    "    labels = [sample[2] for sample in dataset.samples]\n",
    "    class_counts = np.bincount(labels)\n",
    "    class_weights = 10.0 / class_counts\n",
    "    \n",
    "    print(class_weights)\n",
    "    \n",
    "    sample_weights = [class_weights[label] for label in labels]\n",
    "    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00479616 0.04065041]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.sampler.WeightedRandomSampler at 0x1fa267e1e90>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sampler(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=16\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.2)),\n",
    "    transforms.RandomRotation(degrees=20),\n",
    "    transforms.RandomPerspective(distortion_scale=0.3, p=0.5),\n",
    "    # transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    \n",
    "])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = DefectDataset(TRAIN_DIR, transform=img_transform, mask_transform=mask_transform)\n",
    "test_dataset = DefectDataset(TEST_DIR, transform=img_transform, mask_transform=mask_transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, sampler=get_sampler(train_dataset), batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(tensor):\n",
    "    img = tensor.permute(1,2,0)\n",
    "    plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aaadi\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\aaadi\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "resnet = models.resnet50(pretrained=True)\n",
    "\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefectDetector(nn.Module):\n",
    "    def __init__(self, backbone, num_classes=1, freeze_backbone=True):\n",
    "        super(DefectDetector, self).__init__()\n",
    "        \n",
    "        self.backbone = backbone(pretrained=True)\n",
    "        \n",
    "        if freeze_backbone:\n",
    "            for param in list(self.backbone.parameters())[:-10]:\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        in_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "\n",
    "    def fit(self, dataloader, loss_fn, optimizer, epochs=50, lr=0.001):\n",
    "        criterion = loss_fn()\n",
    "        optim = optimizer(self.classifier.parameters(), weight_decay=1e-3, lr=lr)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for images, masks, labels in dataloader:\n",
    "                images, labels = images.to(device), labels.float().to(device)\n",
    "                \n",
    "                optim.zero_grad()\n",
    "                outputs = self.forward(images).squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "            print(f\"Epoch [{epoch + 1}/{epochs}]\")\n",
    "            print(f\"Loss: {total_loss / len(train_dataloader):.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aaadi\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\aaadi\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DefectDetector(\n",
       "  (backbone): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Linear(in_features=512, out_features=1, bias=True)\n",
       "    (7): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DefectDetector(backbone=models.resnet50).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]\n",
      "Loss: 0.616858\n",
      "Epoch [2/50]\n",
      "Loss: 0.557212\n",
      "Epoch [3/50]\n",
      "Loss: 0.547285\n",
      "Epoch [4/50]\n",
      "Loss: 0.531708\n",
      "Epoch [5/50]\n",
      "Loss: 0.525891\n",
      "Epoch [6/50]\n",
      "Loss: 0.524633\n",
      "Epoch [7/50]\n",
      "Loss: 0.512431\n",
      "Epoch [8/50]\n",
      "Loss: 0.520627\n",
      "Epoch [9/50]\n",
      "Loss: 0.521685\n",
      "Epoch [10/50]\n",
      "Loss: 0.511092\n",
      "Epoch [11/50]\n",
      "Loss: 0.509294\n",
      "Epoch [12/50]\n",
      "Loss: 0.502378\n",
      "Epoch [13/50]\n",
      "Loss: 0.517523\n",
      "Epoch [14/50]\n",
      "Loss: 0.518013\n",
      "Epoch [15/50]\n",
      "Loss: 0.508759\n",
      "Epoch [16/50]\n",
      "Loss: 0.505923\n",
      "Epoch [17/50]\n",
      "Loss: 0.506705\n",
      "Epoch [18/50]\n",
      "Loss: 0.516043\n",
      "Epoch [19/50]\n",
      "Loss: 0.505999\n",
      "Epoch [20/50]\n",
      "Loss: 0.531183\n",
      "Epoch [21/50]\n",
      "Loss: 0.512016\n",
      "Epoch [22/50]\n",
      "Loss: 0.486920\n",
      "Epoch [23/50]\n",
      "Loss: 0.502113\n",
      "Epoch [24/50]\n",
      "Loss: 0.498745\n",
      "Epoch [25/50]\n",
      "Loss: 0.486511\n",
      "Epoch [26/50]\n",
      "Loss: 0.497793\n",
      "Epoch [27/50]\n",
      "Loss: 0.510043\n",
      "Epoch [28/50]\n",
      "Loss: 0.496641\n",
      "Epoch [29/50]\n",
      "Loss: 0.497239\n",
      "Epoch [30/50]\n",
      "Loss: 0.486335\n",
      "Epoch [31/50]\n",
      "Loss: 0.495226\n",
      "Epoch [32/50]\n",
      "Loss: 0.499590\n",
      "Epoch [33/50]\n",
      "Loss: 0.507275\n",
      "Epoch [34/50]\n",
      "Loss: 0.504616\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCELoss\n\u001b[0;32m      2\u001b[0m optim \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW\n\u001b[1;32m----> 3\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_dataloader, loss_fn, optim, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0004\u001b[39m)\n",
      "Cell \u001b[1;32mIn[40], line 38\u001b[0m, in \u001b[0;36mDefectDetector.fit\u001b[1;34m(self, dataloader, loss_fn, optimizer, epochs, lr)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     36\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, masks, labels \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m     39\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     41\u001b[0m     optim\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\aaadi\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\aaadi\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\aaadi\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\aaadi\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[12], line 29\u001b[0m, in \u001b[0;36mDefectDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Apply transforms\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 29\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_transform:\n\u001b[0;32m     31\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_transform(mask)\n",
      "File \u001b[1;32mc:\\Users\\aaadi\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\aaadi\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\aaadi\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aaadi\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:805\u001b[0m, in \u001b[0;36mRandomPerspective.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp:\n\u001b[0;32m    804\u001b[0m     startpoints, endpoints \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params(width, height, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistortion_scale)\n\u001b[1;32m--> 805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mperspective(img, startpoints, endpoints, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpolation, fill)\n\u001b[0;32m    806\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\aaadi\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\transforms\\functional.py:737\u001b[0m, in \u001b[0;36mperspective\u001b[1;34m(img, startpoints, endpoints, interpolation, fill)\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    736\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m--> 737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_pil\u001b[38;5;241m.\u001b[39mperspective(img, coeffs, interpolation\u001b[38;5;241m=\u001b[39mpil_interpolation, fill\u001b[38;5;241m=\u001b[39mfill)\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mperspective(img, coeffs, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, fill\u001b[38;5;241m=\u001b[39mfill)\n",
      "File \u001b[1;32mc:\\Users\\aaadi\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:330\u001b[0m, in \u001b[0;36mperspective\u001b[1;34m(img, perspective_coeffs, interpolation, fill)\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be PIL Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    328\u001b[0m opts \u001b[38;5;241m=\u001b[39m _parse_fill(fill, img)\n\u001b[1;32m--> 330\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mtransform(img\u001b[38;5;241m.\u001b[39msize, Image\u001b[38;5;241m.\u001b[39mPERSPECTIVE, perspective_coeffs, interpolation, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopts)\n",
      "File \u001b[1;32mc:\\Users\\aaadi\\anaconda3\\envs\\torch\\Lib\\site-packages\\PIL\\Image.py:2722\u001b[0m, in \u001b[0;36mImage.transform\u001b[1;34m(self, size, method, data, resample, fill, fillcolor)\u001b[0m\n\u001b[0;32m   2718\u001b[0m         im\u001b[38;5;241m.\u001b[39m__transformer(\n\u001b[0;32m   2719\u001b[0m             box, \u001b[38;5;28mself\u001b[39m, Transform\u001b[38;5;241m.\u001b[39mQUAD, quad, resample, fillcolor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2720\u001b[0m         )\n\u001b[0;32m   2721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2722\u001b[0m     im\u001b[38;5;241m.\u001b[39m__transformer(\n\u001b[0;32m   2723\u001b[0m         (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m size, \u001b[38;5;28mself\u001b[39m, method, data, resample, fillcolor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2724\u001b[0m     )\n\u001b[0;32m   2726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m im\n",
      "File \u001b[1;32mc:\\Users\\aaadi\\anaconda3\\envs\\torch\\Lib\\site-packages\\PIL\\Image.py:2805\u001b[0m, in \u001b[0;36mImage.__transformer\u001b[1;34m(self, box, image, method, data, resample, fill)\u001b[0m\n\u001b[0;32m   2802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   2803\u001b[0m     resample \u001b[38;5;241m=\u001b[39m Resampling\u001b[38;5;241m.\u001b[39mNEAREST\n\u001b[1;32m-> 2805\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim\u001b[38;5;241m.\u001b[39mtransform2(box, image\u001b[38;5;241m.\u001b[39mim, method, data, resample, fill)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_fn = t.nn.BCELoss\n",
    "optim = t.optim.AdamW\n",
    "model.fit(train_dataloader, loss_fn, optim, epochs=50, lr=0.0004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DefectDetector(\n",
       "  (backbone): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Linear(in_features=512, out_features=1, bias=True)\n",
       "    (7): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]], device='cuda:0', grad_fn=<RoundBackward0>)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.forward(sample[0][:25].to(device)).round()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1], device='cuda:0',\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predictions[0] == sample[2][:25].to(device)).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "def evaluate(model, test_dataloader, verbose=False):\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    model.eval()\n",
    "    with t.no_grad():\n",
    "        for images, masks, labels in test_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images).round()\n",
    "            preds = outputs.cpu().numpy()\n",
    "            labels = labels.cpu().numpy()\n",
    "\n",
    "            all_labels.extend(labels)\n",
    "            all_preds.extend(preds)\n",
    "\n",
    "            # Compute metrics for each batch\n",
    "            acc = accuracy_score(labels, preds)\n",
    "            if verbose:\n",
    "                print(f\"Batch Accuracy: {acc:.4f}\")\n",
    "                print(classification_report(labels, preds))\n",
    "                cm = confusion_matrix(labels, preds)\n",
    "                disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "                disp.plot()\n",
    "                plt.show()\n",
    "\n",
    "    # Compute metrics for all data combined\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    overall_acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Overall Accuracy: {overall_acc:.4f}\")\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "    overall_cm = confusion_matrix(all_labels, all_preds)\n",
    "    overall_disp = ConfusionMatrixDisplay(confusion_matrix=overall_cm)\n",
    "    overall_disp.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.7819\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.81      0.87       894\n",
      "           1       0.26      0.55      0.35       110\n",
      "\n",
      "    accuracy                           0.78      1004\n",
      "   macro avg       0.60      0.68      0.61      1004\n",
      "weighted avg       0.86      0.78      0.81      1004\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
